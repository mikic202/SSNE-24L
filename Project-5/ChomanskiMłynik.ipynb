{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikic202/miniconda3/envs/pt/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mikic202/miniconda3/envs/pt/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import transformers\n",
    "from transformers import *\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torch.utils.data import Subset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikic202/miniconda3/envs/pt/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/mikic202/.cache/huggingface/hub/models--juliensimon--reviews-sentiment-analysis/snapshots/7d147bc6fbf417d17abf65b4cefe3e04cbc4e7c5/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"juliensimon/reviews-sentiment-analysis\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/mikic202/.cache/huggingface/hub/models--juliensimon--reviews-sentiment-analysis/snapshots/7d147bc6fbf417d17abf65b4cefe3e04cbc4e7c5/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/mikic202/.cache/huggingface/hub/models--juliensimon--reviews-sentiment-analysis/snapshots/7d147bc6fbf417d17abf65b4cefe3e04cbc4e7c5/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/mikic202/.cache/huggingface/hub/models--juliensimon--reviews-sentiment-analysis/snapshots/7d147bc6fbf417d17abf65b4cefe3e04cbc4e7c5/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/mikic202/.cache/huggingface/hub/models--juliensimon--reviews-sentiment-analysis/snapshots/7d147bc6fbf417d17abf65b4cefe3e04cbc4e7c5/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/mikic202/.cache/huggingface/hub/models--juliensimon--reviews-sentiment-analysis/snapshots/7d147bc6fbf417d17abf65b4cefe3e04cbc4e7c5/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"juliensimon/reviews-sentiment-analysis\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/mikic202/.cache/huggingface/hub/models--juliensimon--reviews-sentiment-analysis/snapshots/7d147bc6fbf417d17abf65b4cefe3e04cbc4e7c5/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"juliensimon/reviews-sentiment-analysis\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/mikic202/.cache/huggingface/hub/models--juliensimon--reviews-sentiment-analysis/snapshots/7d147bc6fbf417d17abf65b4cefe3e04cbc4e7c5/model.safetensors\n",
      "Some weights of the model checkpoint at juliensimon/reviews-sentiment-analysis were not used when initializing DistilBertModel: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at juliensimon/reviews-sentiment-analysis.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'juliensimon/reviews-sentiment-analysis'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        words_to_delete = int(len(data['input_ids']) * 0.98)\n",
    "        text_indecies, _ = random_split(\n",
    "            range(len(data['input_ids'])), [words_to_delete, len(data['input_ids']) - words_to_delete]\n",
    "        )\n",
    "        return {'rating': data['rating'], 'input_ids': data['input_ids'][text_indecies], 'attention_mask': data['attention_mask'][text_indecies]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['review', 'rating'],\n",
      "    num_rows: 16392\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "review_dataset = load_dataset(\"csv\", data_files=\"data/train_data.csv\", split=\"train\")\n",
    "print(review_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"review\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = review_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"review\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "torch.manual_seed(73)\n",
    "n_train_examples = int(len(tokenized_datasets) * 0.9)\n",
    "train_indices, validation_indices = random_split(\n",
    "    range(len(tokenized_datasets)), [n_train_examples, len(tokenized_datasets) - n_train_examples]\n",
    ")\n",
    "\n",
    "validation_dataset = Subset(tokenized_datasets, validation_indices)\n",
    "train_dataset = CustomDataset(Subset(tokenized_datasets, train_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['rating', 'input_ids', 'attention_mask'])\n",
      "486\n",
      "tensor([    0,     0, 12271,  2204,     0,     0,  1010,     0])\n",
      "tensor([   0,    0, 2204])\n",
      "486\n",
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataset:\n",
    "    print(i.keys())\n",
    "    print(len(i['input_ids']))\n",
    "    print(i['input_ids'][0:8])\n",
    "    print(i['input_ids'][[0, 1, 3]])\n",
    "    print(len(i['attention_mask']))\n",
    "    print(i['rating'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_train_loader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model(**tokenizer(\"BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.\", return_tensors=\"pt\"))\n",
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0498, -0.3156, -0.3395,  ..., -1.6786, -0.4018, -0.0752],\n",
       "          [ 0.3794, -0.0872, -0.4121,  ..., -1.6896, -0.3334,  0.0322],\n",
       "          [ 0.5837, -0.7698, -0.4135,  ..., -1.5526, -0.4573,  0.1456],\n",
       "          ...,\n",
       "          [ 0.0703, -0.2677, -0.1546,  ..., -1.1223,  0.1195, -0.0484],\n",
       "          [-0.0977, -0.2706,  0.0462,  ..., -1.3017, -0.3355, -0.1699],\n",
       "          [ 0.0298, -0.4435, -0.3852,  ..., -1.2285, -0.2555, -0.4443]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " torch.Size([1, 28, 768]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.last_hidden_state, result.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/mikic202/.cache/huggingface/hub/models--juliensimon--reviews-sentiment-analysis/snapshots/7d147bc6fbf417d17abf65b4cefe3e04cbc4e7c5/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"juliensimon/reviews-sentiment-analysis\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/mikic202/.cache/huggingface/hub/models--juliensimon--reviews-sentiment-analysis/snapshots/7d147bc6fbf417d17abf65b4cefe3e04cbc4e7c5/model.safetensors\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at juliensimon/reviews-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = Adam(sentiment_model.parameters(), lr=3e-5)\n",
    "sentiment_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 922/922 [14:31<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6121862951399447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 922/922 [14:47<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4793394688767363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for batch in tqdm(review_train_loader):\n",
    "        # print(batch)\n",
    "        labels = batch[\"rating\"].to(device)\n",
    "        batch = {\"attention_mask\": batch['attention_mask'].to(device), \"input_ids\": batch['input_ids'].to(device)}#, \"token_type_ids\":batch['token_type_ids'].to(device)}\n",
    "        outputs = sentiment_model(**batch)\n",
    "        loss = loss_fun(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "#         lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "#         progress_bar.update(1)\n",
    "        losses.append(loss.item())\n",
    "    print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6201219512195122}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in validation_dataloader:\n",
    "    labels = batch[\"rating\"].to(device)\n",
    "    batch = {\"attention_mask\": batch['attention_mask'].to(device), \"input_ids\": batch['input_ids'].to(device)}#, \"token_type_ids\":batch['token_type_ids'].to(device)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = sentiment_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=labels)\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sentiment_model.state_dict(),\"sentiment_model_distilbert-base-uncased-go-emotions-student_67.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wszystkie modele były testowane na lr = 5e-5 oraz przy 2-5 epokach\n",
    "\n",
    "\n",
    "Model: distilbert-base-uncased \n",
    "Accuracy: 91,5 na zbiorze treningowym\n",
    "\n",
    "\n",
    "Model: cross-encoder/ms-marco-MiniLM-L-12-v2 \n",
    "Accuracy: 94,25 na zbiorze treningowym\n",
    "Accuracy: 61 na validacyjnym\n",
    "\n",
    "\n",
    "Model: bhadresh-savani/distilbert-base-uncased-emotion \n",
    "Accuracy: 97,7 na zbiorze treningowym\n",
    "\n",
    "\n",
    "Model: joeddav/distilbert-base-uncased-go-emotions-student \n",
    "Accuracy: 97,3  na zbiorze treningowym\n",
    "Accuracy: 67,5 na walidacyjnym\n",
    "\n",
    "\n",
    "Model: finiteautomata/bertweet-base-sentiment-analysis \n",
    "Accuracy: 92,2 na zbiorze treningowym, szybki \n",
    "Po douczeniu Accuracy 97,3\n",
    "\n",
    "\n",
    "Model: bhadresh-savani/bert-base-uncased-emotion \n",
    "Accuracy: 96, 44 na zbiorze treningowym \n",
    "\n",
    "\n",
    "Model: michellejieli/emotion_text_classifier \n",
    "Accuracy 91,37 na zbiorze treningowym\n",
    "\n",
    "\n",
    "Model: Falconsai/intent_classification \n",
    "Accuracy: 96,34 na zbiorze treningowym\n",
    "\n",
    "\n",
    "Model: alperiox/autonlp-user-review-classification-536415182 \n",
    "Accuracy: 93,67 na zbiorze treningowym\n",
    "\n",
    "\n",
    "Model: Jorgeutd/sagemaker-roberta-base-emotion \n",
    "Accuracy: 44,186\n",
    "\n",
    "\n",
    "Model: JungleLee/bert-toxic-comment-classification'\n",
    "Accuracy: 96,59 na zbiorze treningowym\n",
    "\n",
    "\n",
    "Model: nickwong64/bert-base-uncased-poems-sentiment\n",
    "Accuracy: 63,53 na zbiorze walidacyjnym\n",
    "\n",
    "\n",
    "Model: jitesh/emotion-english\n",
    "Accuracy: 65 na walidacyjnym\n",
    "\n",
    "Model: juliensimon/reviews-sentiment-analysis \n",
    "Accuracy: 63 na walidacyjnym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenize_function(examples):\n",
    "    return tokenizer(examples, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['review'],\n",
      "    num_rows: 4099\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4099/4099 [00:01<00:00, 3635.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_review_dataset = load_dataset(\"csv\", data_files=\"data/test_data.csv\", split=\"train\")\n",
    "print(test_review_dataset)\n",
    "\n",
    "test_tokenized_datasets = test_review_dataset.map(tokenize_function, batched=True)\n",
    "test_tokenized_datasets.set_format(\"torch\")\n",
    "test_dataloader = DataLoader(test_tokenized_datasets, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = {\"attention_mask\": batch['attention_mask'].to(device), \"input_ids\": batch['input_ids'].to(device)}#, \"token_type_ids\":batch['token_type_ids'].to(device)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = sentiment_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    test_predictions.extend(predictions.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 3, 4, 4, 0, 3, 4, 1, 4, 3, 3, 4, 3, 4, 4, 1, 3, 3, 3, 4, 0, 4, 1, 4, 4, 4, 4, 2, 1, 4, 2, 4, 2, 3, 4, 4, 1, 4, 3, 4, 4, 3, 3, 1, 4, 2, 4, 4, 4, 3, 3, 4, 2, 4, 2, 4, 4, 2, 4, 4, 1, 4, 4, 4, 3, 3, 4, 4, 3, 4, 2, 2, 4, 3, 1, 4, 4, 2, 4, 4, 3, 1, 4, 4, 3, 4, 4, 3, 3, 0, 3, 4, 3, 0, 4, 4, 4, 0, 4, 4, 4, 3, 3, 0, 2, 4, 3, 2, 4, 3, 3, 3, 0, 4, 3, 4, 2, 4, 4, 4, 4, 3, 4, 4, 1, 2, 4, 4, 0, 2, 4, 3, 1, 1, 3, 3, 4, 4, 0, 4, 2, 4, 4, 3, 3, 4, 4, 4, 1, 3, 4, 4, 4, 3, 3, 4, 4, 3, 1, 2, 4, 3, 2, 1, 4, 4, 2, 4, 4, 1, 4, 3, 4, 0, 4, 3, 3, 4, 0, 3, 4, 4, 4, 4, 1, 4, 0, 4, 4, 2, 2, 3, 3, 4, 4, 3, 3, 1, 3, 1, 3, 3, 3, 0, 3, 1, 4, 1, 1, 1, 4, 1, 4, 3, 4, 3, 3, 4, 4, 4, 2, 4, 1, 2, 3, 4, 3, 4, 3, 3, 3, 3, 1, 1, 3, 4, 4, 4, 4, 1, 1, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 3, 3, 4, 1, 3, 3, 4, 4, 4, 1, 1, 1, 1, 4, 0, 4, 3, 3, 1, 4, 3, 3, 3, 0, 3, 2, 4, 4, 4, 4, 4, 3, 4, 1, 4, 3, 3, 3, 4, 3, 1, 0, 1, 4, 0, 3, 4, 4, 1, 0, 4, 4, 4, 1, 3, 3, 4, 0, 4, 4, 3, 3, 3, 4, 3, 3, 2, 2, 3, 3, 3, 4, 3, 4, 4, 0, 4, 3, 4, 2, 2, 3, 4, 2, 4, 0, 4, 3, 3, 4, 4, 4, 4, 4, 4, 3, 4, 3, 1, 4, 3, 4, 4, 4, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 4, 2, 4, 0, 3, 3, 4, 4, 1, 3, 4, 4, 3, 4, 4, 3, 3, 0, 4, 4, 3, 4, 3, 4, 3, 3, 4, 1, 3, 3, 4, 1, 3, 3, 4, 3, 3, 3, 0, 3, 1, 3, 1, 4, 4, 4, 3, 0, 4, 4, 4, 2, 4, 0, 4, 4, 3, 1, 1, 4, 0, 3, 3, 4, 4, 4, 2, 2, 0, 0, 4, 4, 4, 4, 3, 4, 1, 0, 3, 4, 4, 3, 2, 4, 4, 4, 3, 3, 4, 2, 3, 4, 4, 4, 4, 2, 4, 3, 4, 4, 3, 0, 2, 0, 2, 3, 4, 1, 3, 4, 3, 3, 2, 3, 4, 2, 4, 3, 3, 3, 0, 1, 3, 3, 4, 0, 0, 2, 4, 4, 3, 4, 1, 3, 3, 4, 3, 4, 3, 1, 3, 0, 3, 4, 1, 4, 4, 1, 3, 3, 4, 4, 4, 4, 4, 3, 0, 3, 3, 4, 4, 3, 4, 4, 4, 4, 4, 3, 4, 3, 4, 1, 4, 4, 1, 4, 2, 3, 1, 4, 4, 4, 4, 3, 3, 4, 4, 1, 4, 2, 2, 4, 3, 4, 4, 4, 0, 4, 4, 3, 4, 3, 4, 4, 4, 4, 1, 3, 3, 3, 3, 0, 4, 3, 4, 4, 3, 4, 4, 4, 3, 3, 3, 1, 4, 3, 4, 4, 4, 2, 4, 3, 3, 4, 1, 3, 3, 3, 1, 4, 1, 4, 4, 4, 4, 3, 2, 3, 2, 3, 4, 3, 2, 4, 4, 3, 2, 4, 2, 3, 0, 4, 4, 4, 1, 4, 3, 4, 4, 1, 4, 0, 4, 3, 0, 4, 2, 1, 4, 4, 2, 4, 3, 3, 3, 3, 3, 1, 4, 4, 3, 4, 3, 4, 4, 0, 4, 3, 1, 4, 4, 4, 2, 1, 4, 4, 3, 3, 3, 4, 3, 4, 4, 4, 4, 1, 4, 3, 4, 3, 2, 4, 3, 4, 3, 4, 4, 4, 4, 1, 4, 2, 4, 3, 4, 4, 3, 3, 3, 4, 4, 1, 2, 1, 1, 0, 0, 1, 3, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 1, 4, 3, 3, 0, 4, 2, 3, 4, 1, 2, 3, 4, 4, 0, 4, 3, 4, 1, 3, 0, 4, 1, 4, 3, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 1, 3, 3, 0, 4, 3, 4, 3, 4, 4, 4, 3, 4, 4, 4, 3, 1, 4, 4, 3, 3, 4, 2, 4, 4, 0, 4, 4, 4, 4, 2, 3, 3, 4, 4, 4, 3, 4, 4, 4, 3, 0, 4, 1, 4, 3, 4, 4, 3, 3, 3, 4, 3, 4, 3, 4, 3, 3, 1, 2, 2, 4, 3, 3, 4, 3, 3, 4, 3, 1, 4, 3, 3, 4, 1, 4, 0, 1, 4, 0, 4, 3, 3, 4, 4, 4, 1, 4, 3, 4, 4, 3, 4, 4, 4, 4, 4, 4, 3, 0, 4, 3, 3, 4, 3, 3, 3, 4, 4, 3, 0, 4, 4, 3, 4, 4, 4, 2, 3, 4, 4, 4, 3, 2, 4, 3, 4, 4, 4, 4, 1, 4, 3, 3, 1, 3, 1, 3, 4, 0, 4, 4, 4, 4, 3, 3, 4, 1, 3, 4, 4, 1, 4, 4, 2, 1, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 3, 4, 2, 3, 3, 3, 3, 2, 3, 1, 1, 3, 4, 4, 3, 1, 3, 3, 4, 3, 3, 4, 3, 1, 0, 3, 0, 4, 4, 1, 3, 3, 0, 3, 0, 4, 3, 3, 2, 3, 3, 3, 3, 4, 4, 1, 4, 4, 2, 4, 2, 4, 3, 3, 4, 4, 0, 0, 1, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 4, 3, 4, 0, 4, 3, 3, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 1, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 2, 3, 4, 4, 4, 3, 4, 3, 0, 3, 1, 4, 4, 3, 3, 4, 4, 4, 1, 3, 4, 4, 3, 4, 4, 1, 4, 3, 3, 3, 4, 4, 1, 3, 3, 1, 1, 3, 4, 4, 3, 4, 1, 4, 4, 4, 2, 1, 3, 0, 3, 4, 4, 4, 3, 0, 4, 4, 3, 1, 4, 4, 4, 4, 4, 2, 1, 4, 4, 4, 4, 4, 4, 3, 4, 0, 1, 4, 4, 4, 3, 4, 3, 1, 2, 3, 4, 0, 3, 4, 4, 4, 4, 3, 3, 1, 4, 4, 2, 4, 1, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 3, 3, 3, 3, 4, 2, 4, 2, 3, 2, 1, 2, 4, 3, 1, 3, 0, 1, 0, 1, 1, 1, 2, 2, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 1, 4, 0, 4, 2, 0, 4, 3, 4, 3, 3, 4, 4, 4, 3, 3, 4, 4, 3, 4, 4, 4, 4, 4, 3, 4, 3, 4, 4, 4, 3, 3, 3, 3, 2, 4, 3, 4, 2, 4, 1, 0, 3, 3, 3, 4, 4, 4, 4, 1, 4, 4, 0, 4, 3, 3, 2, 4, 3, 4, 1, 4, 3, 3, 1, 4, 4, 4, 3, 4, 3, 1, 3, 3, 1, 4, 4, 4, 2, 4, 4, 3, 3, 4, 3, 4, 0, 0, 4, 4, 1, 2, 2, 4, 4, 1, 4, 4, 0, 4, 3, 4, 3, 4, 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 1, 3, 3, 3, 3, 4, 0, 2, 4, 3, 0, 4, 3, 3, 4, 4, 1, 3, 4, 3, 3, 3, 3, 4, 2, 3, 3, 4, 4, 3, 3, 4, 4, 1, 0, 3, 0, 4, 4, 3, 2, 4, 3, 3, 4, 4, 3, 4, 3, 4, 4, 2, 4, 1, 4, 3, 0, 3, 2, 2, 0, 4, 4, 2, 4, 4, 4, 3, 4, 3, 4, 4, 4, 3, 3, 4, 4, 4, 4, 3, 1, 2, 4, 1, 2, 3, 0, 3, 4, 0, 4, 1, 3, 4, 3, 4, 0, 3, 3, 4, 4, 4, 4, 4, 0, 4, 4, 0, 4, 3, 0, 4, 1, 4, 3, 4, 4, 4, 3, 3, 4, 1, 3, 3, 3, 3, 4, 3, 2, 3, 3, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 3, 3, 4, 4, 3, 4, 0, 3, 4, 4, 0, 0, 3, 4, 4, 1, 4, 3, 3, 4, 3, 3, 4, 2, 1, 3, 4, 4, 2, 3, 2, 1, 3, 3, 4, 4, 4, 4, 2, 3, 3, 3, 3, 4, 4, 3, 4, 3, 4, 3, 4, 1, 3, 1, 4, 3, 3, 4, 4, 3, 3, 4, 4, 4, 3, 4, 4, 4, 3, 4, 3, 3, 1, 2, 0, 4, 4, 4, 3, 2, 1, 4, 2, 4, 4, 4, 3, 2, 3, 4, 4, 4, 4, 4, 3, 3, 3, 3, 0, 1, 4, 3, 3, 1, 4, 1, 2, 2, 3, 2, 3, 4, 4, 4, 0, 1, 3, 4, 3, 4, 4, 4, 1, 4, 4, 4, 3, 4, 3, 3, 4, 3, 2, 4, 3, 3, 4, 2, 4, 4, 1, 4, 0, 4, 4, 4, 2, 4, 4, 4, 3, 3, 4, 2, 3, 2, 3, 4, 4, 4, 4, 3, 1, 1, 0, 4, 4, 3, 1, 4, 3, 2, 3, 3, 3, 0, 1, 4, 3, 4, 4, 4, 4, 3, 3, 4, 3, 1, 3, 2, 3, 2, 3, 4, 3, 0, 3, 4, 3, 2, 2, 1, 4, 3, 1, 3, 4, 4, 3, 4, 4, 3, 3, 3, 1, 3, 1, 3, 0, 3, 4, 3, 4, 2, 4, 3, 4, 4, 2, 4, 4, 3, 4, 3, 4, 1, 4, 3, 1, 4, 0, 4, 3, 3, 1, 4, 2, 4, 4, 4, 4, 4, 4, 3, 3, 3, 4, 4, 4, 3, 4, 3, 4, 1, 4, 4, 3, 1, 4, 2, 4, 3, 4, 2, 3, 4, 4, 4, 3, 4, 2, 1, 4, 3, 2, 4, 4, 4, 1, 4, 3, 3, 1, 4, 2, 2, 0, 4, 4, 4, 0, 4, 4, 4, 1, 2, 4, 4, 1, 3, 4, 3, 3, 0, 2, 1, 3, 4, 3, 3, 4, 0, 3, 4, 2, 2, 4, 4, 3, 3, 4, 4, 3, 4, 4, 3, 4, 1, 3, 2, 4, 0, 4, 2, 1, 3, 4, 3, 0, 4, 1, 3, 3, 3, 4, 3, 4, 4, 3, 4, 0, 1, 4, 4, 3, 4, 4, 3, 4, 3, 4, 4, 3, 4, 4, 2, 4, 4, 4, 4, 3, 4, 4, 4, 1, 3, 2, 4, 3, 3, 4, 2, 4, 4, 4, 3, 4, 3, 4, 3, 4, 3, 3, 4, 4, 4, 4, 4, 3, 4, 3, 3, 2, 1, 2, 4, 4, 3, 4, 3, 3, 4, 3, 3, 4, 3, 1, 4, 4, 4, 4, 1, 4, 4, 3, 2, 3, 3, 3, 4, 0, 0, 4, 4, 3, 4, 3, 4, 3, 4, 3, 4, 4, 4, 4, 0, 3, 3, 3, 3, 3, 2, 4, 4, 4, 4, 0, 4, 1, 4, 4, 3, 4, 3, 3, 4, 4, 3, 4, 4, 4, 4, 1, 4, 4, 2, 4, 3, 3, 3, 4, 4, 3, 4, 4, 4, 1, 4, 3, 3, 4, 1, 3, 3, 4, 0, 4, 1, 4, 4, 4, 4, 3, 1, 4, 3, 3, 3, 0, 3, 3, 3, 2, 4, 4, 1, 4, 3, 4, 2, 3, 4, 0, 0, 4, 4, 4, 0, 4, 4, 3, 3, 4, 4, 1, 3, 4, 3, 4, 0, 4, 3, 0, 4, 0, 2, 3, 2, 0, 3, 3, 4, 3, 0, 0, 2, 4, 4, 4, 3, 4, 4, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 2, 1, 4, 4, 3, 4, 4, 4, 4, 2, 4, 4, 3, 4, 3, 3, 4, 4, 3, 3, 1, 4, 4, 2, 4, 1, 2, 4, 3, 4, 4, 3, 3, 4, 4, 4, 0, 3, 1, 3, 1, 4, 3, 3, 0, 3, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 1, 1, 4, 4, 4, 4, 4, 3, 4, 3, 1, 3, 3, 4, 0, 4, 4, 4, 4, 0, 4, 4, 3, 4, 4, 3, 4, 3, 3, 4, 3, 1, 3, 3, 3, 4, 4, 4, 3, 4, 3, 1, 4, 3, 4, 3, 3, 4, 4, 1, 4, 3, 4, 4, 4, 1, 4, 2, 3, 4, 4, 4, 4, 4, 4, 3, 1, 3, 4, 3, 4, 1, 0, 3, 4, 4, 4, 4, 3, 1, 3, 4, 4, 4, 3, 4, 4, 4, 2, 4, 1, 4, 4, 3, 1, 0, 3, 4, 3, 3, 4, 4, 2, 4, 4, 4, 4, 4, 3, 4, 1, 4, 4, 4, 4, 4, 4, 3, 3, 4, 4, 4, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 2, 4, 4, 3, 4, 3, 0, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 4, 3, 4, 3, 4, 1, 4, 0, 3, 4, 4, 4, 2, 1, 4, 4, 3, 4, 0, 3, 4, 2, 2, 2, 3, 2, 4, 4, 4, 4, 4, 3, 4, 3, 4, 3, 4, 3, 4, 4, 4, 4, 0, 4, 4, 2, 4, 4, 3, 3, 3, 3, 4, 3, 4, 4, 3, 2, 4, 4, 4, 4, 3, 4, 4, 3, 3, 3, 3, 4, 0, 4, 3, 3, 4, 2, 4, 3, 3, 4, 0, 4, 4, 2, 3, 2, 4, 4, 3, 3, 4, 0, 4, 3, 4, 0, 2, 3, 4, 4, 4, 1, 4, 4, 4, 4, 0, 3, 2, 0, 4, 4, 4, 3, 4, 0, 2, 1, 4, 4, 4, 4, 3, 4, 4, 3, 4, 2, 4, 4, 3, 4, 4, 1, 1, 3, 4, 3, 3, 2, 4, 4, 3, 4, 4, 3, 4, 3, 2, 4, 2, 4, 3, 4, 4, 4, 3, 4, 4, 3, 4, 0, 4, 4, 4, 3, 3, 3, 1, 3, 3, 1, 4, 4, 3, 1, 4, 4, 2, 4, 3, 2, 3, 4, 4, 4, 4, 4, 4, 3, 2, 4, 1, 4, 4, 4, 4, 3, 0, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 0, 2, 4, 0, 4, 3, 3, 2, 4, 4, 3, 4, 3, 4, 3, 3, 4, 4, 4, 3, 3, 4, 4, 4, 4, 3, 3, 1, 4, 4, 4, 4, 4, 1, 4, 4, 1, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 1, 3, 4, 4, 3, 1, 4, 4, 0, 4, 4, 4, 3, 4, 3, 4, 4, 4, 0, 4, 3, 4, 4, 3, 3, 4, 3, 4, 2, 4, 4, 4, 0, 4, 3, 3, 3, 2, 3, 3, 4, 4, 3, 3, 4, 3, 3, 3, 2, 4, 3, 4, 4, 3, 3, 3, 4, 0, 4, 3, 1, 3, 3, 4, 4, 3, 3, 2, 4, 0, 3, 3, 0, 4, 4, 1, 4, 3, 4, 1, 4, 4, 2, 3, 4, 3, 4, 3, 4, 3, 3, 4, 3, 2, 4, 4, 1, 3, 4, 4, 3, 3, 4, 0, 1, 3, 3, 3, 3, 0, 3, 3, 3, 4, 4, 3, 3, 3, 4, 4, 4, 3, 0, 4, 1, 4, 3, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 4, 3, 4, 4, 4, 4, 0, 3, 1, 4, 0, 4, 1, 3, 3, 3, 3, 4, 4, 3, 4, 4, 3, 4, 4, 2, 4, 4, 3, 1, 4, 3, 3, 4, 4, 4, 0, 4, 4, 4, 1, 4, 4, 4, 4, 1, 4, 3, 1, 4, 4, 4, 4, 0, 4, 3, 3, 3, 4, 2, 3, 3, 4, 4, 4, 3, 3, 4, 4, 4, 2, 3, 3, 4, 3, 4, 4, 4, 4, 1, 3, 4, 4, 3, 3, 3, 4, 4, 3, 4, 3, 4, 4, 4, 0, 1, 4, 3, 3, 4, 2, 4, 4, 4, 3, 4, 4, 4, 3, 4, 0, 3, 4, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 2, 4, 4, 3, 4, 3, 4, 4, 4, 4, 4, 3, 4, 1, 3, 3, 0, 4, 4, 3, 4, 0, 2, 3, 4, 2, 3, 4, 4, 4, 4, 3, 4, 0, 3, 0, 4, 3, 1, 4, 4, 4, 2, 1, 4, 3, 2, 3, 3, 4, 2, 3, 4, 1, 0, 1, 4, 4, 3, 1, 4, 4, 4, 2, 2, 4, 0, 3, 4, 4, 0, 4, 4, 4, 4, 4, 0, 0, 3, 4, 3, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 3, 4, 1, 0, 3, 4, 3, 4, 2, 4, 4, 3, 3, 3, 4, 4, 0, 3, 3, 4, 3, 4, 3, 3, 2, 2, 4, 1, 4, 4, 4, 3, 4, 3, 4, 3, 4, 0, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 3, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 3, 4, 1, 4, 4, 4, 3, 3, 4, 3, 3, 4, 2, 4, 4, 3, 4, 4, 3, 4, 4, 3, 1, 4, 3, 3, 4, 3, 3, 4, 1, 4, 4, 1, 3, 1, 3, 3, 4, 4, 0, 1, 1, 4, 4, 4, 1, 4, 3, 4, 3, 4, 4, 2, 4, 3, 1, 4, 3, 3, 4, 4, 4, 3, 4, 3, 4, 0, 4, 3, 4, 4, 1, 0, 4, 3, 3, 3, 3, 4, 4, 4, 3, 3, 0, 4, 3, 3, 1, 3, 4, 3, 4, 4, 4, 4, 2, 4, 1, 4, 4, 3, 4, 3, 1, 3, 4, 4, 0, 4, 3, 4, 1, 3, 2, 2, 4, 4, 4, 3, 1, 3, 2, 4, 3, 4, 3, 3, 2, 3, 2, 4, 4, 4, 2, 3, 1, 3, 0, 4, 3, 4, 4, 1, 3, 4, 4, 3, 2, 2, 4, 4, 3, 4, 4, 4, 1, 2, 3, 4, 4, 3, 4, 3, 0, 4, 4, 0, 0, 3, 3, 4, 3, 3, 3, 0, 4, 3, 3, 3, 4, 1, 4, 4, 2, 4, 3, 4, 0, 3, 3, 3, 3, 4, 3, 4, 3, 1, 4, 4, 0, 1, 4, 4, 4, 4, 4, 4, 4, 2, 3, 1, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, 4, 3, 4, 4, 4, 4, 4, 3, 0, 4, 0, 3, 4, 4, 3, 4, 0, 4, 4, 4, 4, 4, 3, 2, 3, 4, 4, 1, 2, 1, 3, 3, 3, 3, 4, 4, 3, 4, 4, 3, 4, 4, 3, 0, 4, 4, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 1, 4, 4, 4, 4, 1, 4, 0, 3, 0, 3, 4, 4, 1, 3, 4, 3, 3, 3, 4, 4, 3, 4, 4, 3, 3, 3, 4, 2, 4, 2, 3, 3, 4, 4, 1, 4, 4, 3, 3, 3, 3, 4, 2, 4, 3, 4, 3, 3, 3, 3, 4, 3, 2, 4, 4, 3, 3, 4, 4, 2, 4, 1, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 4, 4, 3, 4, 4, 3, 2, 4, 4, 3, 4, 4, 4, 3, 3, 3, 4, 3, 4, 4, 3, 4, 0, 4, 4, 4, 3, 3, 1, 1, 3, 4, 3, 2, 4, 4, 2, 4, 1, 4, 3, 3, 1, 3, 4, 3, 3, 3, 4, 3, 3, 3, 3, 3, 1, 3, 3, 4, 4, 3, 2, 0, 1, 4, 3, 4, 4, 3, 4, 3, 4, 2, 3, 4, 3, 4, 3, 3, 4, 2, 4, 0, 4, 4, 4, 3, 3, 2, 3, 0, 2, 4, 3, 4, 4, 4, 3, 3, 4, 3, 2, 4, 3, 3, 4, 0, 4, 3, 4, 4, 4, 4, 0, 0, 4, 3, 4, 3, 1, 4, 3, 4, 2, 4, 4, 4, 4, 3, 3, 2, 4, 1, 2, 4, 3, 3, 3, 4, 4, 1, 3, 3, 3, 3, 3, 4, 2, 0, 2, 1, 4, 3, 2, 3, 4, 4, 2, 3, 4, 1, 1, 4, 4, 3, 3, 3, 4, 3, 1, 3, 4, 4, 4, 4, 4, 3, 4, 2, 4, 3, 3, 4, 3, 3, 4, 3, 0, 4, 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, 3, 4, 2, 4, 1, 3, 0, 3, 4, 3, 4, 4, 4, 1, 4, 2, 4, 2, 4, 4, 3, 4, 4, 4, 4, 4, 4, 0, 1, 4, 4, 1, 2, 4, 3, 4, 4, 4, 4, 1, 0, 1, 4, 4, 3, 4, 4, 4, 3, 3, 4, 3, 4, 0, 1, 4, 4, 4, 2, 2, 2, 3, 4, 4, 3, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 3, 3, 3, 3, 4, 3, 4, 0, 4, 4, 3, 1, 3, 3, 4, 4, 1, 4, 3, 0, 4, 3, 4, 4, 4, 4, 1, 0, 3, 4, 3, 2, 1, 3, 1, 2, 4, 1, 2, 4, 4, 2, 3, 3, 4, 0, 4, 3, 4, 4, 3, 3, 3, 4, 3, 3, 3, 3, 4, 0, 4, 4, 3, 4, 3, 1, 4, 4, 2, 4, 1, 4, 0, 4, 4, 4, 4, 2, 2, 0, 4, 2, 4, 4, 0, 4, 3, 3, 4, 2, 3, 3, 2, 2, 3, 3, 4, 3, 4, 0, 3, 4, 2, 3, 4, 3, 4, 3, 4, 1, 4, 4, 3, 1, 4, 0, 1, 1, 4, 3, 2, 4, 4, 4, 1, 4, 4, 4, 1, 4, 4, 3, 3, 0, 1, 4, 4, 4, 2, 3, 4, 4, 4, 4, 3, 3, 0, 4, 4, 3, 1, 3, 3, 4, 3, 4, 4, 4, 4, 3, 3, 3, 4, 4, 1, 4, 1, 3, 2, 4, 3, 3, 3, 4, 4, 4, 3, 1, 3, 2, 3, 1, 4, 4, 4, 4, 3, 4, 3, 3, 3, 3, 1, 4, 4, 2, 4, 4, 3, 3, 3, 1, 3, 4, 0, 4, 2, 3, 1, 3, 4, 4, 4, 3, 4, 3, 3, 4, 4, 2, 4, 3, 1, 4, 3, 3, 3, 2, 4, 3, 4, 3, 3, 4, 1, 4, 3, 3, 3, 3, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 4, 3, 1, 2, 0, 1, 3, 3, 1, 2, 3, 4, 4, 3, 3, 3, 4, 3, 4, 4, 3, 2, 0, 3, 4, 4, 4, 2, 3, 4, 4, 1, 4, 3, 3, 4, 4, 3, 4, 3, 3, 0, 3, 4, 1, 3, 0, 4, 3, 4, 2, 1, 4, 4, 4, 3, 3, 1, 4, 4, 4, 1, 3, 4, 3, 4, 2, 4, 4, 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 1, 4, 1, 4, 3, 3, 4, 4, 3, 1, 4, 4, 2, 4, 3, 4, 3, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 1, 4, 3, 3, 4, 4, 4, 4, 4, 3, 2, 4, 4, 3, 0, 2, 1, 4, 3, 3, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 3, 4, 0, 1, 3, 3, 4, 4, 3, 3, 3, 4, 4, 4, 4, 1, 0, 4, 4, 3, 4, 4, 4, 4, 4, 3, 3, 4, 3, 1, 2, 4, 3, 3, 1, 4, 4, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 0, 4, 4, 2, 3, 3, 3, 4, 4, 3, 3, 4, 4, 4, 4, 3, 4, 4, 4, 3, 3, 3, 1, 3, 4, 4, 3, 4, 4, 4, 0, 4, 2, 3, 4, 4, 3, 4, 4, 1, 3, 4, 4, 4, 4, 4, 4, 3, 1, 0, 4, 4, 4, 3, 3, 2, 2, 1, 3, 3, 4, 2, 4, 0, 3, 3, 4, 3, 2, 1, 1, 4, 3, 4, 3, 3, 4, 4, 4, 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 4, 1, 4, 4, 0, 4, 3, 3, 2, 3, 0, 4, 4, 3, 0]\n",
      "4099\n",
      "(array([0, 1, 2, 3, 4]), array([ 234,  344,  285, 1253, 1983]))\n"
     ]
    }
   ],
   "source": [
    "test_predictions\n",
    "print(test_predictions)\n",
    "print(len(test_predictions))\n",
    "print(np.unique(test_predictions, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_predictions).to_csv(\"ChomanskiMłynik_distilbert-base-uncased-go-emotions-student_67.csv\", index=False, header=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
