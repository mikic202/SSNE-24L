{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikic202/miniconda3/envs/pt/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mikic202/miniconda3/envs/pt/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import transformers\n",
    "from transformers import *\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torch.utils.data import Subset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/mikic202/.cache/huggingface/hub/models--finiteautomata--bertweet-base-sentiment-analysis/snapshots/924fc4c80bccb8003d21fe84dd92c7887717f245/vocab.txt\n",
      "loading file bpe.codes from cache at /home/mikic202/.cache/huggingface/hub/models--finiteautomata--bertweet-base-sentiment-analysis/snapshots/924fc4c80bccb8003d21fe84dd92c7887717f245/bpe.codes\n",
      "loading file added_tokens.json from cache at /home/mikic202/.cache/huggingface/hub/models--finiteautomata--bertweet-base-sentiment-analysis/snapshots/924fc4c80bccb8003d21fe84dd92c7887717f245/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /home/mikic202/.cache/huggingface/hub/models--finiteautomata--bertweet-base-sentiment-analysis/snapshots/924fc4c80bccb8003d21fe84dd92c7887717f245/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/mikic202/.cache/huggingface/hub/models--finiteautomata--bertweet-base-sentiment-analysis/snapshots/924fc4c80bccb8003d21fe84dd92c7887717f245/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at None\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "loading configuration file config.json from cache at /home/mikic202/.cache/huggingface/hub/models--finiteautomata--bertweet-base-sentiment-analysis/snapshots/924fc4c80bccb8003d21fe84dd92c7887717f245/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"finiteautomata/bertweet-base-sentiment-analysis\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEG\",\n",
      "    \"1\": \"NEU\",\n",
      "    \"2\": \"POS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"NEG\": 0,\n",
      "    \"NEU\": 1,\n",
      "    \"POS\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/mikic202/.cache/huggingface/hub/models--finiteautomata--bertweet-base-sentiment-analysis/snapshots/924fc4c80bccb8003d21fe84dd92c7887717f245/pytorch_model.bin\n",
      "Some weights of the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'finiteautomata/bertweet-base-sentiment-analysis'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        words_to_delete = int(len(data['input_ids']) * 0.99)\n",
    "        text_indecies, _ = random_split(\n",
    "            range(len(data['input_ids'])), [words_to_delete, len(data['input_ids']) - words_to_delete]\n",
    "        )\n",
    "        return {'rating': data['rating'], 'input_ids': data['input_ids'][text_indecies], 'attention_mask': data['attention_mask'][text_indecies]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['review', 'rating'],\n",
      "    num_rows: 16392\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "review_dataset = load_dataset(\"csv\", data_files=\"data/train_data.csv\", split=\"train\")\n",
    "print(review_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"review\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = review_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"review\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "torch.manual_seed(73)\n",
    "n_train_examples = int(len(tokenized_datasets) * 0.9)\n",
    "train_indices, validation_indices = random_split(\n",
    "    range(len(tokenized_datasets)), [n_train_examples, len(tokenized_datasets) - n_train_examples]\n",
    ")\n",
    "\n",
    "validation_dataset = Subset(tokenized_datasets, validation_indices)\n",
    "train_dataset = CustomDataset(Subset(tokenized_datasets, train_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['rating', 'input_ids', 'attention_mask'])\n",
      "486\n",
      "tensor([    0,     0, 12271,  2204,     0,     0,  1010,     0])\n",
      "tensor([   0,    0, 2204])\n",
      "486\n",
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataset:\n",
    "    print(i.keys())\n",
    "    print(len(i['input_ids']))\n",
    "    print(i['input_ids'][0:8])\n",
    "    print(i['input_ids'][[0, 1, 3]])\n",
    "    print(len(i['attention_mask']))\n",
    "    print(i['rating'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_train_loader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model(**tokenizer(\"BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.\", return_tensors=\"pt\"))\n",
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0498, -0.3156, -0.3395,  ..., -1.6786, -0.4018, -0.0752],\n",
       "          [ 0.3794, -0.0872, -0.4121,  ..., -1.6896, -0.3334,  0.0322],\n",
       "          [ 0.5837, -0.7698, -0.4135,  ..., -1.5526, -0.4573,  0.1456],\n",
       "          ...,\n",
       "          [ 0.0703, -0.2677, -0.1546,  ..., -1.1223,  0.1195, -0.0484],\n",
       "          [-0.0977, -0.2706,  0.0462,  ..., -1.3017, -0.3355, -0.1699],\n",
       "          [ 0.0298, -0.4435, -0.3852,  ..., -1.2285, -0.2555, -0.4443]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " torch.Size([1, 28, 768]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.last_hidden_state, result.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/mikic202/.cache/huggingface/hub/models--finiteautomata--bertweet-base-sentiment-analysis/snapshots/924fc4c80bccb8003d21fe84dd92c7887717f245/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"finiteautomata/bertweet-base-sentiment-analysis\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/mikic202/.cache/huggingface/hub/models--finiteautomata--bertweet-base-sentiment-analysis/snapshots/924fc4c80bccb8003d21fe84dd92c7887717f245/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(130, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = Adam(sentiment_model.parameters(), lr=6e-5)\n",
    "sentiment_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 922/922 [07:53<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7082111597739952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 922/922 [07:59<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5866529792304799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for batch in tqdm(review_train_loader):\n",
    "        # print(batch)\n",
    "        labels = batch[\"rating\"].to(device)\n",
    "        batch = {\"attention_mask\": batch['attention_mask'].to(device), \"input_ids\": batch['input_ids'].to(device)}#, \"token_type_ids\":batch['token_type_ids'].to(device)}\n",
    "        outputs = sentiment_model(**batch)\n",
    "        loss = loss_fun(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "#         lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "#         progress_bar.update(1)\n",
    "        losses.append(loss.item())\n",
    "    print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5768292682926829}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in validation_dataloader:\n",
    "    labels = batch[\"rating\"].to(device)\n",
    "    batch = {\"attention_mask\": batch['attention_mask'].to(device), \"input_ids\": batch['input_ids'].to(device)}#, \"token_type_ids\":batch['token_type_ids'].to(device)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = sentiment_model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=labels)\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sentiment_model.state_dict(),\"sentiment_model_dict_emotion-english_65.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: distilbert-base-uncased \n",
    "Accuracy: 91,5 na zbiorze treningowym\n",
    "\n",
    "\n",
    "Model: cross-encoder/ms-marco-MiniLM-L-12-v2 \n",
    "Accuracy: 94,25 na zbiorze treningowym\n",
    "Accuracy: 61 na validacyjnym\n",
    "\n",
    "\n",
    "Model: bhadresh-savani/distilbert-base-uncased-emotion \n",
    "Accuracy: 97,7 na zbiorze treningowym\n",
    "\n",
    "\n",
    "Model: joeddav/distilbert-base-uncased-go-emotions-student \n",
    "Accuracy: 97,3  na zbiorze treningowym\n",
    "Accuracy: 66,5\n",
    "\n",
    "\n",
    "Model: finiteautomata/bertweet-base-sentiment-analysis \n",
    "Accuracy: 92,2 na zbiorze treningowym, szybki \n",
    "Po douczeniu Accuracy 97,3\n",
    "\n",
    "\n",
    "Model: bhadresh-savani/bert-base-uncased-emotion \n",
    "Accuracy: 96, 44 na zbiorze treningowym \n",
    "Bardziej skomplikowany więc może mniej się przeuczył\n",
    "\n",
    "\n",
    "Model: michellejieli/emotion_text_classifier \n",
    "Accuracy 91,37 na zbiorze treningowym\n",
    "\n",
    "\n",
    "Model: Falconsai/intent_classification \n",
    "Accuracy: 96,34 na zbiorze treningowym\n",
    "\n",
    "\n",
    "Model: alperiox/autonlp-user-review-classification-536415182 \n",
    "Accuracy: 93,67 na zbiorze treningowym\n",
    "\n",
    "\n",
    "Model: Jorgeutd/sagemaker-roberta-base-emotion \n",
    "Accuracy: 44,186\n",
    "\n",
    "\n",
    "Model: JungleLee/bert-toxic-comment-classification'\n",
    "Accuracy: 96,59 na zbiorze treningowym\n",
    "\n",
    "\n",
    "Model: nickwong64/bert-base-uncased-poems-sentiment\n",
    "Accuracy: 63,53 na zbiorze walidacyjnym\n",
    "\n",
    "\n",
    "Model: jitesh/emotion-english\n",
    "Accuracy: 65 na walidacyjnym"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
